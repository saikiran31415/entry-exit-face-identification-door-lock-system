{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5kpS-qn1jWC",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import pathlib\n",
        "def detect_objects(weights, img_size, conf_threshold, source, frame_delay):\n",
        "    # Load YOLOv5 model\n",
        "    pathlib.PosixPath = pathlib.WindowsPath\n",
        "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=weights)\n",
        "\n",
        "    # Set device (GPU or CPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Set inference size\n",
        "    imgsz = img_size\n",
        "\n",
        "    # Open video file\n",
        "    cap = cv2.VideoCapture(source)\n",
        "\n",
        "    # Initialize flag to indicate if object has been detected\n",
        "    object_detected = False\n",
        "    detection_frame_number = -1\n",
        "\n",
        "    # Process video frame by frame\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Perform inference\n",
        "        results = model(frame, size=imgsz)\n",
        "\n",
        "        # Process detection results\n",
        "        for result in results.xyxy[0]:\n",
        "            label = int(result[5])\n",
        "            confidence = float(result[4])\n",
        "\n",
        "            # Check if object is detected with confidence above threshold\n",
        "            if confidence >= conf_threshold:\n",
        "                object_detected = True\n",
        "                detection_frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "                break\n",
        "\n",
        "        # Stop processing if object is detected\n",
        "        if object_detected:\n",
        "            break\n",
        "\n",
        "    # Release video capture object\n",
        "    cap.release()\n",
        "\n",
        "    if object_detected:\n",
        "        # Calculate the frame number to capture after the detection frame\n",
        "        capture_frame_number = detection_frame_number + frame_delay\n",
        "\n",
        "        # Re-open video file to capture the desired frame\n",
        "        cap = cv2.VideoCapture(source)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, capture_frame_number)\n",
        "\n",
        "        # Read and capture the frame\n",
        "        ret, captured_frame = cap.read()\n",
        "        if ret:\n",
        "            # Save the captured frame as PNG with higher quality\n",
        "            output_path = Path(source).parent / f\"captured_frame_.png\"\n",
        "            # Specify higher quality (0-9), default is 3\n",
        "            cv2.imwrite(str(output_path), captured_frame, [cv2.IMWRITE_PNG_COMPRESSION, 9])\n",
        "            print(f\"Captured frame {capture_frame_number} after object detection. Saved as {output_path}\")\n",
        "        else:\n",
        "            print(f\"Failed to capture frame {capture_frame_number} after object detection.\")\n",
        "\n",
        "        # Release video capture object\n",
        "        cap.release()\n",
        "    else:\n",
        "        print(\"Object not detected in the video.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojnfat5x2YXx",
        "outputId": "aaec0593-e086-4ab9-fb65-6007facc3cfb"
      },
      "outputs": [],
      "source": [
        "# YOLOv5 PyTorch HUB Inference (DetectionModels only)\n",
        "import torch\n",
        "\n",
        "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True, trust_repo=True)  # or yolov5n - yolov5x6 or custom\n",
        "model = torch.hub.load(str(Path('yolov5')), 'yolov5s', source='local')\n",
        "im = 'https://ultralytics.com/images/zidane.jpg'  # file, Path, PIL.Image, OpenCV, nparray, list\n",
        "results = model(im)  # inference\n",
        "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6GOgk8i2cL5",
        "outputId": "5360c873-8963-4557-e602-4e4daf3e69fc"
      },
      "outputs": [],
      "source": [
        "  # Perform object detection and frame capture\n",
        "detect_objects('./content/exp5/weights/best.pt', 1080, 0.30, './content/videos/20250418_132955.mp4', 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crop and Align"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U0T93uP6N4Z"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiP6X9yY6QUY"
      },
      "outputs": [],
      "source": [
        "class CardAlignment:\n",
        "    def __init__(self, visualization=False):\n",
        "        self.visualization_mode = visualization\n",
        "\n",
        "    def crop_card(self, image):\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
        "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        max_contour = max(contours, key=cv2.contourArea)\n",
        "        rect = cv2.minAreaRect(max_contour)\n",
        "        box = cv2.boxPoints(rect)\n",
        "        box = np.intp(box)\n",
        "        card = image.copy()\n",
        "        cv2.drawContours(card, [box], 0, (0, 255, 0), 2)\n",
        "        (x, y, w, h) = cv2.boundingRect(box)\n",
        "        card = card[y:y+h, x:x+w]\n",
        "        return card, rect\n",
        "\n",
        "    def align_card(self, card, rect_angle):\n",
        "        height, width = card.shape[:2]\n",
        "\n",
        "        # Correct angle\n",
        "        if rect_angle < -45:\n",
        "            rect_angle += 90\n",
        "        elif rect_angle > 45:\n",
        "            rect_angle -= 90\n",
        "\n",
        "        # Rotate to fix small tilt\n",
        "        rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), rect_angle, 1)\n",
        "        aligned_card = cv2.warpAffine(card, rotation_matrix, (width, height), flags=cv2.INTER_LINEAR)\n",
        "        aligned_card = cv2.rotate(aligned_card, cv2.ROTATE_90_CLOCKWISE)\n",
        "        # After rotation, check if card is vertical\n",
        "        aligned_height, aligned_width = aligned_card.shape[:2]\n",
        "        if aligned_height > aligned_width:\n",
        "            # Rotate 90 degrees to make it horizontal\n",
        "            aligned_card = cv2.rotate(aligned_card, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "\n",
        "        return aligned_card"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zaLRYMT6T_H"
      },
      "outputs": [],
      "source": [
        "# Function to print the image\n",
        "def plot_image(image):\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Output Image')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOXY9hIK6WBn"
      },
      "outputs": [],
      "source": [
        "def process_image(image):\n",
        "    card_aligner = CardAlignment(visualization=True)\n",
        "    card, rect = card_aligner.crop_card(image)\n",
        "    rect_angle = rect[-1]\n",
        "    aligned_card = card_aligner.align_card(card, rect_angle)\n",
        "\n",
        "    # Check if additional cropping is required\n",
        "    gray_aligned = cv2.cvtColor(aligned_card, cv2.COLOR_BGR2GRAY)\n",
        "    _, thresh_aligned = cv2.threshold(gray_aligned, 1, 255, cv2.THRESH_BINARY)\n",
        "    contours_aligned, _ = cv2.findContours(thresh_aligned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    max_contour_aligned = max(contours_aligned, key=cv2.contourArea)\n",
        "    rect_aligned = cv2.minAreaRect(max_contour_aligned)\n",
        "    box_aligned = cv2.boxPoints(rect_aligned)\n",
        "    box_aligned = np.intp(box_aligned)\n",
        "\n",
        "    # Calculate bounding box\n",
        "    (x, y, w, h) = cv2.boundingRect(box_aligned)\n",
        "\n",
        "    # Apply additional cropping if required\n",
        "    if x > 0 or y > 0 or x + w < aligned_card.shape[1] or y + h < aligned_card.shape[0]:\n",
        "        aligned_card = aligned_card[y:y+h, x:x+w]\n",
        "\n",
        "    plot_image(aligned_card)\n",
        "    return aligned_card"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "AyNFdFD46YNv",
        "outputId": "83c236df-bd0f-4fe4-8ab7-f5eb9ec229f9"
      },
      "outputs": [],
      "source": [
        "# Path to the image file\n",
        "original_image = cv2.imread(\"./content/videos/captured_frame_.png\")\n",
        "plot_image(original_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "4mAVJu5-6azf",
        "outputId": "0ee01cb2-b916-4397-ca7a-ecc18120ed00"
      },
      "outputs": [],
      "source": [
        "# Process the image\n",
        "aligned_image = process_image(original_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "42RXZ3lW64gP",
        "outputId": "93a3ab64-59b4-44c7-c726-0c91c75e13ab"
      },
      "outputs": [],
      "source": [
        " \n",
        "aligned_image1 = aligned_image.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO0Crrz767Q4",
        "outputId": "3148e427-4e87-4f0b-c5ed-90bef54222dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytesseract in c:\\users\\rkeya\\desktop\\automated-entry-exit-system-using-image-processing-main\\automated-entry-exit-system-using-image-processing-main\\env\\lib\\site-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in c:\\users\\rkeya\\desktop\\automated-entry-exit-system-using-image-processing-main\\automated-entry-exit-system-using-image-processing-main\\env\\lib\\site-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\rkeya\\desktop\\automated-entry-exit-system-using-image-processing-main\\automated-entry-exit-system-using-image-processing-main\\env\\lib\\site-packages (from pytesseract) (11.2.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3p75F4g68B9",
        "outputId": "de92e672-76b2-4f0c-800c-0d7efcc9a7a5"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install tesseract-ocr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image and  Text Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PjRt7mh6_dI"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "# pytesseract.pytesseract.tesseract_cmd = \"C:\\Program Files\\Tesseract-OCR\"\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "import datetime\n",
        "import pytz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPImZ52G7CgX",
        "outputId": "fe7dda4b-62d7-4bfc-98ab-f5e2d331f0df"
      },
      "outputs": [],
      "source": [
        "custom_config = r'--oem 3 --psm 6'\n",
        "\n",
        "# OCR directly on your existing image\n",
        "text = pytesseract.image_to_string(aligned_image1, config=custom_config)\n",
        "\n",
        "\n",
        "# Optional: See full text detected\n",
        "print(\"Full OCR Text:\\n\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McBGHSitHyIG",
        "outputId": "953c7ff1-824a-4e80-d1bd-dc5bf84d4510"
      },
      "outputs": [],
      "source": [
        "# Extract Roll Number\n",
        "roll_number = None\n",
        "for line in text.split('\\n'):\n",
        "    if \"Roll\" in line or \"Roll No\" in line:\n",
        "        print(\"Line found:\", line)\n",
        "        # Extract only digits\n",
        "        numbers = ''.join(filter(str.isdigit, line))\n",
        "        if numbers:\n",
        "            roll_number = numbers\n",
        "            break\n",
        "\n",
        "# Output Roll Number\n",
        "if roll_number:\n",
        "    print(f\"\\nExtracted Roll Number: {roll_number}\")\n",
        "else:\n",
        "    print(\"\\nRoll Number not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oKhDUDiMO0r"
      },
      "outputs": [],
      "source": [
        "img_copy = aligned_image1.copy()\n",
        "# Get image dimensions\n",
        "height, width, _ = img_copy.shape\n",
        "# # Draw the rectangle on the image\n",
        "green_color = (0, 255, 0)  # Green color in BGR\n",
        "border_thickness = 2  # Thickness of the border\n",
        "top_left = (int(width * (.1/70)), int(height * (1/4)))\n",
        "bottom_right = (int(width * (1/3.2)), int(height * (3/4)))\n",
        "\n",
        "# Define shrinkage percentages\n",
        "shrink_percentage = 0.06\n",
        "\n",
        "# Calculate the shrinkage amount\n",
        "shrink_x = int((bottom_right[0] - top_left[0]) * shrink_percentage)\n",
        "shrink_y = int((bottom_right[1] - top_left[1]) * shrink_percentage)\n",
        "\n",
        "# Adjust the coordinates to shrink the rectangle\n",
        "top_left = (top_left[0] + shrink_x, top_left[1] + shrink_y)\n",
        "bottom_right = (bottom_right[0] - shrink_x, bottom_right[1] - shrink_y)\n",
        "cv2.rectangle(img_copy, top_left, bottom_right, green_color, border_thickness)\n",
        "# Convert BGR image to RGB for displaying with matplotlib\n",
        "img_rgb = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmJVCwYyMRV7"
      },
      "outputs": [],
      "source": [
        "cropped_img = img_rgb[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "fz_brccbMUSk",
        "outputId": "7eff3385-9688-42d8-b2ca-b6c1b7010541"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Display the cropped image\n",
        "def plot_image(img, title='Cropped Image'):\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(cropped_img)\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "plot_image(cropped_img)\n",
        "img = Image.fromarray(cropped_img)\n",
        "img.save('./content/cropped_image/id_image.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Face Recognition and Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import face_recognition\n",
        "\n",
        "# List of paths to known images\n",
        "known_image_paths = [\"./content/live_image/vishnu2.jpg\"]\n",
        "\n",
        "# List of corresponding names or labels\n",
        "known_names = [\"Match\"]\n",
        "\n",
        "# Initialize lists to store known face encodings and names\n",
        "known_face_encodings = []\n",
        "known_face_names = []\n",
        "\n",
        "# Iterate over the known image paths and names\n",
        "for image_path, name in zip(known_image_paths, known_names):\n",
        "    # Load the known image\n",
        "    known_image = face_recognition.load_image_file(image_path)\n",
        "\n",
        "    # Encode the face in the known image\n",
        "    face_encoding = face_recognition.face_encodings(known_image)[0]\n",
        "\n",
        "    # Append the face encoding and name to the lists\n",
        "    known_face_encodings.append(face_encoding)\n",
        "    known_face_names.append(name)\n",
        "\n",
        "# Now you have lists of known face encodings and corresponding names\n",
        "# You can use these lists for face recognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the query image\n",
        "import requests\n",
        "import time\n",
        "\n",
        "nodemcu_ip = \"http://192.168.126.151\"  # replace with your NodeMCU's IP\n",
        "\n",
        "def trigger_servo():\n",
        "    try:\n",
        "        response = requests.get(nodemcu_ip + \"/move\")\n",
        "        print(\"Response from NodeMCU:\", response.text)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "\n",
        "query_image = face_recognition.load_image_file(\"./content/cropped_image/id_image.png\")\n",
        "query_face_encoding = face_recognition.face_encodings(query_image)[0]\n",
        "\n",
        "# Compare encodings with known faces\n",
        "for known_face_encoding, known_face_name in zip(known_face_encodings, known_face_names):\n",
        "    # Compare face encoding of query image with known face encodings\n",
        "    matches = face_recognition.compare_faces([known_face_encoding], query_face_encoding)\n",
        "\n",
        "    if any(matches):\n",
        "        valid_rolls = [\"244101042\",\"244101043\" ]\n",
        "        if roll_number in valid_rolls:\n",
        "            print(f\"Access Granted\")\n",
        "            trigger_servo()\n",
        "        else:\n",
        "            print(\"Access Denied : Invalid Roll Number\")\n",
        "            trigger_servo()\n",
        "            trigger_servo()\n",
        "        \n",
        "    else:\n",
        "        print(\"Access Denied : Face Mismatch\")\n",
        "        trigger_servo()\n",
        "        trigger_servo()\n",
        "        trigger_servo()\n",
        "        \n",
        "\n",
        "        # Additional processing or actions can be performed here\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
